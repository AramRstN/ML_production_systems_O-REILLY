{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Chapter 3: Feature Engineering and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers the critical processes of **feature engineering** and **feature selection**, which lie at the heart of building effective, scalable machine learning systems. These processes are essential in both training and inference pipelines, especially in production ML environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå Why Feature Engineering Matters\n",
    "\n",
    "> *\"Coming up with features is difficult, time-consuming, and requires expert knowledge.\"* ‚Äî Andrew Ng\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that models can learn from effectively. It directly influences:\n",
    "- Model convergence speed\n",
    "- Predictive performance\n",
    "- Compute and storage efficiency\n",
    "\n",
    "It involves an iterative process of projecting, transforming, reducing, or combining data to extract meaningful signals.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Matching Training and Serving\n",
    "\n",
    "During training, we often use the full dataset to calculate statistics (like standard deviation or means). At serving time, we process each input independently. Any mismatch here introduces **training-serving skew** ‚Äî a critical production bug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_feature_std(feature_column):\n",
    "    return np.std(feature_column)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Common Preprocessing Operations\n",
    "\n",
    "Feature engineering involves several recurring transformations:\n",
    "- Data cleansing\n",
    "- Normalization / Standardization\n",
    "- Bucketizing\n",
    "- One-hot encoding\n",
    "- Dimensionality reduction\n",
    "- Image & text feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "X = [[1], [2], [3]]\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßπ Data Cleaning Example\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\"timestamp\": [\"00:00\", \"12:00\", \"00:00\"]})\n",
    "df_cleaned = df[df[\"timestamp\"] != \"00:00\"]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™£ Bucketizing Numerical Features\n",
    "\n",
    "Bucketizing converts numerical values into ranges (bins), often making patterns easier to model.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "values = pd.Series([1, 2, 2, 3, 4, 5, 6, 7, 8])\n",
    "buckets = pd.qcut(values, q=3, labels=[\"low\", \"medium\", \"high\"])\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ûï Feature Crosses\n",
    "\n",
    "Combining features to capture interactions (e.g. day √ó hour ‚Üí hour of week):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"day\": [1, 2], \"hour\": [10, 12]})\n",
    "df[\"hour_of_week\"] = df[\"day\"] * 24 + df[\"hour\"]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìâ Dimensionality Reduction\n",
    "\n",
    "Used when input space is large but can be compressed (e.g. PCA, UMAP):\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "pca = PCA(n_components=1)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Feature Selection: Why & How\n",
    "\n",
    "Too many features:\n",
    "- Increases training time & cost\n",
    "- Introduces overfitting risk\n",
    "- Slows inference\n",
    "\n",
    "Goal: **Keep only predictive features**.\n",
    "\n",
    "### üìä Filter Methods\n",
    "\n",
    "```python\n",
    "cor = df.corr()\n",
    "cor_target = abs(cor[\"target\"])\n",
    "selected_features = cor_target[cor_target > 0.8].index\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Univariate Selection\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def univariate_selection(X, y):\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "    X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "    selector = SelectKBest(score_func=chi2, k=10)\n",
    "    selector.fit(X_scaled, y_train)\n",
    "    return X.columns[selector.get_support()]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ Recursive Feature Elimination (RFE)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_rfe(X, y, k):\n",
    "    model = RandomForestClassifier()\n",
    "    rfe = RFE(model, n_features_to_select=k)\n",
    "    rfe.fit(X, y)\n",
    "    return X.columns[rfe.get_support()]\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üå≤ Embedded Methods\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "selector = SelectFromModel(model, prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Tokenization with TF Transform\n",
    "\n",
    "For LLMs, text data is tokenized into input IDs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "START_TOKEN_ID = 101\n",
    "END_TOKEN_ID = 102\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    tokenizer = tf_text.BertTokenizer(...)\n",
    "    text = tf_text.normalize_utf8(inputs[\"message\"])\n",
    "    tokens = tokenizer.tokenize(text).merge_dims(-2, -1)\n",
    "    tokens, type_ids = tf_text.combine_segments(tokens, START_TOKEN_ID, END_TOKEN_ID)\n",
    "    tokens, mask_ids = tf_text.pad_model_inputs(tokens, max_seq_length=128)\n",
    "    return {\"input_ids\": tokens, \"input_mask\": mask_ids, \"input_type_ids\": type_ids}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Summary Tables\n",
    "\n",
    "### üîß Feature Engineering Keywords & Techniques\n",
    "\n",
    "The process of transforming raw data into input features that help machine learning models learn better and more efficiently. It includes scaling, encoding, aggregating, and creating new features.\n",
    "\n",
    "| **Keyword**               | **Definition**                                                                 |\n",
    "|---------------------------|-------------------------------------------------------------------------------|\n",
    "| Normalization             | Scaling features to a range (e.g., [0,1]) using min-max scaling               |\n",
    "| Standardization           | Rescaling to zero mean and unit variance (z-score)                           |\n",
    "| One-Hot Encoding          | Converting categories to binary indicator vectors                            |\n",
    "| Bucketizing               | Grouping numeric features into discrete bins                                 |\n",
    "| Feature Crosses           | Combining multiple features into one                                          |\n",
    "| Dimensionality Reduction  | Reducing feature space size while preserving information (e.g. PCA, UMAP)     |\n",
    "| Embeddings                | Dense representations for high-cardinality features (e.g. text, categories)  |\n",
    "| Data Cleansing            | Removing or correcting invalid/missing data                                  |\n",
    "| Text Tokenization         | Splitting text into tokens for NLP models                                     |\n",
    "| TF Transform (TFX)        | Scalable, consistent feature transformation library for TensorFlow           |\n",
    "| Training‚ÄìServing Skew     | Discrepancy between training and inference transformations                   |\n",
    "\n",
    "\n",
    "### üß† Feature Selection Keywords & Techniques\n",
    "\n",
    "The process of selecting a subset of relevant features from the original set that contribute the most to model performance, while reducing noise, overfitting, and computational cost.\n",
    "\n",
    "| **Technique**             | **Type**         | **Definition**                                                                 |\n",
    "|---------------------------|------------------|-------------------------------------------------------------------------------|\n",
    "| Filter Methods            | Supervised/Unsupervised | Based on stats like correlation, chi-square                             |\n",
    "| Wrapper Methods           | Supervised       | Evaluate subsets using model performance (e.g., RFE)                         |\n",
    "| Embedded Methods          | Supervised       | Feature selection built into model (e.g., tree importance, L1 penalty)       |\n",
    "| Univariate Selection      | Filter           | Evaluate features independently via stat tests (e.g., chi2, F-test)          |\n",
    "| Recursive Feature Elimination | Wrapper    | Iteratively removes least important features                                |\n",
    "| SelectFromModel           | Embedded         | Keep features with high importance scores from trained model                |\n",
    "| Mutual Information        | Filter           | Measures feature-target dependency                                           |\n",
    "| Forward Selection         | Wrapper          | Add features iteratively based on best improvement                          |\n",
    "| Backward Elimination      | Wrapper          | Remove features iteratively starting from all                               |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Benefits of TensorFlow Transform (TFX)\n",
    "\n",
    "- Prevents training-serving skew\n",
    "- Runs at scale via Apache Beam\n",
    "- Produces graphs + transformed data\n",
    "- Allows preprocessing to be deployed with the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Conclusion\n",
    "\n",
    "- **Feature engineering** enhances learnability and efficiency.\n",
    "- **Feature selection** improves performance and scalability.\n",
    "- Use scalable tools (TFX, Beam) in production.\n",
    "- In GenAI/LLMs, **example selection** is as critical as feature work.\n",
    "\n",
    "> Focus on **data quality over data quantity**. What your model learns depends entirely on what you give it.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ *This notebook helps you implement Chapter 3 of* Machine Learning Production Systems *in real-world pipelines.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
