{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Chapter 3: Feature Engineering and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers the critical processes of **feature engineering** and **feature selection**, which lie at the heart of building effective, scalable machine learning systems. These processes are essential in both training and inference pipelines, especially in production ML environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Why Feature Engineering Matters\n",
    "\n",
    "> *\"Coming up with features is difficult, time-consuming, and requires expert knowledge.\"* â€” Andrew Ng\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that models can learn from effectively. It directly influences:\n",
    "- Model convergence speed\n",
    "- Predictive performance\n",
    "- Compute and storage efficiency\n",
    "\n",
    "It involves an iterative process of projecting, transforming, reducing, or combining data to extract meaningful signals.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Matching Training and Serving\n",
    "\n",
    "During training, we often use the full dataset to calculate statistics (like standard deviation or means). At serving time, we process each input independently. Any mismatch here introduces **training-serving skew** â€” a critical production bug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_feature_std(feature_column):\n",
    "    return np.std(feature_column)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Common Preprocessing Operations\n",
    "\n",
    "Feature engineering involves several recurring transformations:\n",
    "- Data cleansing\n",
    "- Normalization / Standardization\n",
    "- Bucketizing\n",
    "- One-hot encoding\n",
    "- Dimensionality reduction\n",
    "- Image & text feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "X = [[1], [2], [3]]\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§¹ Data Cleaning Example\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\"timestamp\": [\"00:00\", \"12:00\", \"00:00\"]})\n",
    "df_cleaned = df[df[\"timestamp\"] != \"00:00\"]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸª£ Bucketizing Numerical Features\n",
    "\n",
    "Bucketizing converts numerical values into ranges (bins), often making patterns easier to model.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "values = pd.Series([1, 2, 2, 3, 4, 5, 6, 7, 8])\n",
    "buckets = pd.qcut(values, q=3, labels=[\"low\", \"medium\", \"high\"])\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âž• Feature Crosses\n",
    "\n",
    "Combining features to capture interactions (e.g. day Ã— hour â†’ hour of week):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"day\": [1, 2], \"hour\": [10, 12]})\n",
    "df[\"hour_of_week\"] = df[\"day\"] * 24 + df[\"hour\"]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‰ Dimensionality Reduction\n",
    "\n",
    "Used when input space is large but can be compressed (e.g. PCA, UMAP):\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "pca = PCA(n_components=1)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Feature Selection: Why & How\n",
    "\n",
    "Too many features:\n",
    "- Increases training time & cost\n",
    "- Introduces overfitting risk\n",
    "- Slows inference\n",
    "\n",
    "Goal: **Keep only predictive features**.\n",
    "\n",
    "### ðŸ“Š Filter Methods\n",
    "\n",
    "```python\n",
    "cor = df.corr()\n",
    "cor_target = abs(cor[\"target\"])\n",
    "selected_features = cor_target[cor_target > 0.8].index\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª Univariate Selection\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def univariate_selection(X, y):\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "    X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "    selector = SelectKBest(score_func=chi2, k=10)\n",
    "    selector.fit(X_scaled, y_train)\n",
    "    return X.columns[selector.get_support()]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Recursive Feature Elimination (RFE)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_rfe(X, y, k):\n",
    "    model = RandomForestClassifier()\n",
    "    rfe = RFE(model, n_features_to_select=k)\n",
    "    rfe.fit(X, y)\n",
    "    return X.columns[rfe.get_support()]\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ² Embedded Methods\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "selector = SelectFromModel(model, prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Tokenization with TF Transform\n",
    "\n",
    "For LLMs, text data is tokenized into input IDs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "START_TOKEN_ID = 101\n",
    "END_TOKEN_ID = 102\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    tokenizer = tf_text.BertTokenizer(...)\n",
    "    text = tf_text.normalize_utf8(inputs[\"message\"])\n",
    "    tokens = tokenizer.tokenize(text).merge_dims(-2, -1)\n",
    "    tokens, type_ids = tf_text.combine_segments(tokens, START_TOKEN_ID, END_TOKEN_ID)\n",
    "    tokens, mask_ids = tf_text.pad_model_inputs(tokens, max_seq_length=128)\n",
    "    return {\"input_ids\": tokens, \"input_mask\": mask_ids, \"input_type_ids\": type_ids}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Benefits of TensorFlow Transform (TFX)\n",
    "\n",
    "- Prevents training-serving skew\n",
    "- Runs at scale via Apache Beam\n",
    "- Produces graphs + transformed data\n",
    "- Allows preprocessing to be deployed with the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Conclusion\n",
    "\n",
    "- **Feature engineering** enhances learnability and efficiency.\n",
    "- **Feature selection** improves performance and scalability.\n",
    "- Use scalable tools (TFX, Beam) in production.\n",
    "- In GenAI/LLMs, **example selection** is as critical as feature work.\n",
    "\n",
    "> Focus on **data quality over data quantity**. What your model learns depends entirely on what you give it.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… *This notebook helps you implement Chapter 3 of* Machine Learning Production Systems *in real-world pipelines.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
