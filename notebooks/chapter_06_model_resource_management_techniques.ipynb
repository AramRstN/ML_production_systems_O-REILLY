{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Chapter 6: Model Resource Management Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In production ML systemsâ€”especially on mobile, IoT, or serverless environmentsâ€”resources like compute, memory, and energy are limited. \n",
    "\n",
    "This chapter focuses on optimizing machine learning models to be lightweight and efficient, covering techniques like:\n",
    "- **Dimensionality Reduction**: Reduce input complexity\n",
    "- **Quantization & Pruning**: Reduce model size and latency\n",
    "- **Knowledge Distillation**: Retain performance with smaller models\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dimensionality Reduction\n",
    "Reducing the number of input features helps models:\n",
    "- Train faster\n",
    "- Require less memory\n",
    "- Generalize better\n",
    "\n",
    "### Curse of Dimensionality\n",
    "High-dimensional feature spaces lead to:\n",
    "- Sparse data, requiring exponentially more samples\n",
    "- Distance metrics (like Euclidean) becoming less informative\n",
    "- Increased overfitting risk\n",
    "\n",
    "### Solutions\n",
    "- Manual feature selection (domain knowledge)\n",
    "- Feature selection algorithms (correlation, mutual info, RFE)\n",
    "- Dimensionality reduction (e.g., PCA, UMAP)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Word Embedding Example in Keras\n",
    "A Keras example using the Reuters dataset:\n",
    "\n",
    "- We train a model with **1000D embeddings** (large, high-quality but costly)\n",
    "- Then train the same model with **10D embeddings** (cheaper, potentially lower accuracy)\n",
    "\n",
    "Compare performance and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Keras setup and data prep\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words)\n",
    "y_train = to_categorical(y_train, 46)\n",
    "y_test = to_categorical(y_test, 46)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=20)\n",
    "x_test = pad_sequences(x_test, maxlen=20)\n",
    "```\n",
    "\n",
    "```python\n",
    "# High-dimensional embedding model\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(num_words, 1000),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(46, activation='softmax')\n",
    "])\n",
    "```\n",
    "```python\n",
    "# Low-dimensional version\n",
    "model_lowdim = keras.Sequential([\n",
    "    layers.Embedding(num_words, 10),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(46, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA (Principal Component Analysis)\n",
    "PCA is an unsupervised algorithm that:\n",
    "- Finds axes with maximal variance\n",
    "- Projects data onto a lower-dimensional space\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(iris.data)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=iris.target)\n",
    "plt.title(\"PCA on Iris Dataset\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization and Pruning\n",
    "\n",
    "### Quantization\n",
    "Quantization reduces model size by converting:\n",
    "- **float32** weights to **int8** or **float16**\n",
    "- Reduces memory use, improves cache performance\n",
    "\n",
    "Used especially in **TF Lite**, **ONNX**, and **Edge AI**.\n",
    "\n",
    "#### Post-training Quantization\n",
    "```python\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"model_quantized.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "\n",
    "#### Quantization-aware Training\n",
    "Trains the model with simulated quantization noise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "Pruning removes unnecessary weights (e.g. near-zero weights).\n",
    "Benefits:\n",
    "- Fewer operations\n",
    "- Smaller model size\n",
    "- Retains accuracy\n",
    "\n",
    "```python\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "pruning_params = {\n",
    "  'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "    initial_sparsity=0.0,\n",
    "    final_sparsity=0.5,\n",
    "    begin_step=0,\n",
    "    end_step=1000)\n",
    "}\n",
    "\n",
    "model_pruned = prune_low_magnitude(model, **pruning_params)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Knowledge Distillation\n",
    "A smaller **student model** learns to mimic a larger **teacher model**.\n",
    "\n",
    "### Why?\n",
    "- Transfer performance of a large model to a smaller one\n",
    "- Reduce size/latency while retaining accuracy\n",
    "- Used in DistilBERT, TinyML, etc.\n",
    "\n",
    "### Strategy\n",
    "- Use soft targets (probability distributions) from the teacher\n",
    "- Blend with standard supervised loss\n",
    "\n",
    "```python\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def distillation_loss(y_true, y_pred, teacher_pred, temperature=5.0, alpha=0.5):\n",
    "    y_pred_soft = K.softmax(y_pred / temperature)\n",
    "    teacher_soft = K.softmax(teacher_pred / temperature)\n",
    "    kl_loss = K.sum(teacher_soft * K.log(teacher_soft / y_pred_soft), axis=-1)\n",
    "    hard_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return alpha * hard_loss + (1 - alpha) * kl_loss\n",
    "```\n",
    "\n",
    "### Visual: Teacher-Student Architecture\n",
    "```python\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "# Draw Teacher block\n",
    "ax.add_patch(patches.Rectangle((0.1, 0.4), 0.2, 0.3, edgecolor='blue', facecolor='lightblue'))\n",
    "ax.text(0.2, 0.55, 'Teacher Model', ha='center')\n",
    "\n",
    "# Draw arrows\n",
    "ax.annotate('', xy=(0.3, 0.55), xytext=(0.5, 0.55), arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "ax.text(0.4, 0.58, 'Soft Targets')\n",
    "\n",
    "# Draw Student block\n",
    "ax.add_patch(patches.Rectangle((0.5, 0.4), 0.2, 0.3, edgecolor='green', facecolor='lightgreen'))\n",
    "ax.text(0.6, 0.55, 'Student Model', ha='center')\n",
    "\n",
    "# Style\n",
    "ax.axis('off')\n",
    "plt.title('Knowledge Distillation: Teacher â†’ Student')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Real-world examples\n",
    "- **DistilBERT**: Compresses BERT while keeping 95% of its performance\n",
    "- **TinyBERT**, **TMKD**, **Noisy Student**: Use teacher noise, multiple teachers, or synthetic labels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Efficient model deployment requires:\n",
    "- Reducing feature space (dimensionality reduction)\n",
    "- Compressing models (quantization, pruning)\n",
    "- Transferring knowledge (distillation)\n",
    "\n",
    "These techniques help deploy on mobile, embedded, or edge devices at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## Keywords\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Dimensionality Reduction** | Reducing the number of input features while preserving important information. |\n",
    "| **Curse of Dimensionality** | In high dimensions, data becomes sparse and distance metrics lose meaning. |\n",
    "| **PCA** | Principal Component Analysis; projects data into a lower-dimensional space. |\n",
    "| **Embedding** | Vector representation of input features, often used for text or categorical data. |\n",
    "| **Pruning** | Removing weights or neurons that have little impact on output to shrink model size. |\n",
    "| **Quantization** | Reducing precision (e.g., float32 â†’ int8) to improve efficiency. |\n",
    "| **TF Lite** | TensorFlow Lite; optimized framework for deploying models on edge devices. |\n",
    "| **Knowledge Distillation** | Training a smaller model (student) to mimic a larger model (teacher). |\n",
    "| **Teacher/Student Models** | Architecture where the teacher guides the student with soft predictions. |\n",
    "| **KL Divergence** | A loss function measuring difference between predicted distributions. |\n",
    "| **Softmax Temperature** | A technique to soften output distributions to enhance distillation. |\n",
    "| **DistilBERT / TinyBERT / TMKD / Noisy Student** | Real-world examples of distilled or compressed models. |\n",
    "| **Efficient Inference** | Making model predictions faster and lighter in production. |\n",
    "| **Model Compression** | General term for reducing model size (quantization, pruning, etc.). |\n",
    "| **Mobile and Edge ML** | Deploying machine learning models on mobile or embedded devices. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
