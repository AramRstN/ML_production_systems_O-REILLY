{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Chapter 4: Data Journey and Data Storage\n",
    "\n",
    "This chapter focuses on how **data evolves across a production ML pipeline**, and how to manage and validate it with tools like **ML Metadata (MLMD)**, **TensorFlow Metadata (TFMD)**, and **TensorFlow Data Validation (TFDV)**. It also introduces storage patterns like **feature stores**, **data warehouses**, and **data lakes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ Data Journey & Data Provenance\n",
    "\n",
    "**Data journey**: Tracks how data flows and transforms from collection to training and inference.\n",
    "**Data provenance**: Captures the lineage of data, allowing you to trace the origin and changes over time.\n",
    "\n",
    "### üß± Artifacts\n",
    "Objects produced by ML pipeline steps ‚Äì includes raw data, transformed data, models, schemas, metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÉÔ∏è ML Metadata (MLMD)\n",
    "\n",
    "MLMD is a library for storing and retrieving metadata about ML workflows, such as:\n",
    "- Artifacts (inputs/outputs)\n",
    "- Executions (runs of components)\n",
    "- Contexts (e.g. pipeline ID, project ID)\n",
    "\n",
    "```python\n",
    "# MLMD metadata model relationships\n",
    "# Artifact <-- event --> Execution <-- association --> Context\n",
    "```\n",
    "\n",
    "**Usage examples:**\n",
    "- Compare models trained on the same dataset\n",
    "- Trace back how/when data was transformed\n",
    "- Build DAGs of pipeline executions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Using a Schema (TFMD + TFDV)\n",
    "\n",
    "A **schema** defines expectations for input data:\n",
    "- Feature names and types (e.g., int, float, string)\n",
    "- Whether features are required\n",
    "- Expected value ranges or valency\n",
    "\n",
    "Schemas can:\n",
    "- Catch anomalies (missing values, wrong types)\n",
    "- Track data evolution\n",
    "- Guide feature engineering\n",
    "\n",
    "```python\n",
    "# Example: Schema field in TFMD\n",
    "feature {\n",
    "  name: \"some_feature\"\n",
    "  type: BYTES\n",
    "  presence {\n",
    "    min_fraction: 1.0\n",
    "  }\n",
    "  not_in_environment: \"Serving\"\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ TFDV Schema Validation\n",
    "\n",
    "TFDV provides tools like:\n",
    "```python\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "dataset_stats = tfdv.generate_statistics_from_tfrecord(\"train.tfrecord\")\n",
    "schema = tfdv.infer_schema(statistics=dataset_stats)\n",
    "tfdv.display_schema(schema)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Validate against schema\n",
    "anomalies = tfdv.validate_statistics(statistics=dataset_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Schema Environments\n",
    "\n",
    "Schemas can differ across environments (training vs serving):\n",
    "```protobuf\n",
    "default_environment: \"Training\"\n",
    "default_environment: \"Serving\"\n",
    "feature {\n",
    "  name: \"label\"\n",
    "  not_in_environment: \"Serving\"\n",
    "}\n",
    "```\n",
    "This ensures the label is only used during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìâ Detecting Changes Across Datasets\n",
    "\n",
    "TFDV supports:\n",
    "- `skew_comparator`: Compare training vs serving\n",
    "- `drift_comparator`: Detect changes across time\n",
    "\n",
    "```python\n",
    "# Set drift threshold in schema proto\n",
    "drift_comparator: {\n",
    "  infinity_norm_threshold: 0.05\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè¢ Enterprise Data Storage\n",
    "\n",
    "Efficient storage is critical for scalable ML. This chapter compares:\n",
    "\n",
    "### üîß Feature Stores\n",
    "Centralized systems for:\n",
    "- Storing features + metadata\n",
    "- Serving features in real-time or batch\n",
    "- Enabling reuse across teams\n",
    "\n",
    "Feature stores help:\n",
    "- Avoid duplicate feature engineering\n",
    "- Version & time-travel features\n",
    "- Serve features with low latency\n",
    "\n",
    "```python\n",
    "# Conceptual use: store + retrieve feature vectors\n",
    "feature_store.put(\"user_age_bucket\", feature_vector)\n",
    "features = feature_store.get([\"user_age_bucket\"], user_id=\"123\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßä Data Warehouses vs Databases\n",
    "\n",
    "| **Data Warehouse**          | **Database**                    |\n",
    "|-----------------------------|----------------------------------|\n",
    "| Meant for **analytics**     | Meant for **transactions**       |\n",
    "| Historical data, batched    | Real-time operations             |\n",
    "| Schema-on-write             | Schema-on-write                  |\n",
    "| Complex queries             | Fast, simple queries             |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíß Data Lakes vs Warehouses\n",
    "\n",
    "| **Data Lake**                          | **Data Warehouse**                      |\n",
    "|----------------------------------------|------------------------------------------|\n",
    "| Stores **raw** data                    | Stores **processed & structured** data   |\n",
    "| Schema-on-read                         | Schema-on-write                          |\n",
    "| Great flexibility, may become swampy   | High consistency and performance         |\n",
    "\n",
    "**Data Swamp**: When a data lake becomes disorganized and unusable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Keyword & Concept List\n",
    "\n",
    "| **Term**                  | **Definition** |\n",
    "|---------------------------|----------------|\n",
    "| **MLMD**                  | Library to track pipeline metadata (artifacts, runs, etc.) |\n",
    "| **TFMD**                  | TensorFlow Metadata ‚Äì defines schema for features |\n",
    "| **TFDV**                  | TensorFlow Data Validation ‚Äì validates data against schema |\n",
    "| **Schema**                | Structured definition of expected data fields, types, valency |\n",
    "| **Artifact**              | Any output/input of a pipeline component |\n",
    "| **Data Provenance**       | Traceability of data transformations across pipeline |\n",
    "| **Feature Store**         | Central repository of reusable, curated features |\n",
    "| **Data Lake**             | Flexible store for raw or semi-structured data |\n",
    "| **Data Warehouse**        | Structured store for analytics over historical data |\n",
    "| **Skew/Drift**            | Discrepancies in feature distribution over time or between splits |\n",
    "| **Time Travel (ML)**      | Ensuring only past-known features are used during training |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ *This notebook captures the key tools and patterns to manage metadata and storage in production ML pipelines.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
